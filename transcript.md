# Introduction

### Welcome to country

I wish to begin by acknowledging the Wurundjeri people, the traditional owners of the land on which we are gathered today. We pay our respects to the local people for allowing us to have our gathering on their land and to their Elders; past, present and future.

Thanks everyone for coming today, and a huge thanks to Ben and Rick for convening an event as amazing as Buzzconf Nights.

### Audio impaired

If there are any hearing impaired members of the audience, let me know and I can provide you with a transcript of the presentation to follow along to.

### License

This presentation is licensed under Creative Commons universal - you're free to share and re-use this presentation. Please use it to do awesome things.

### Kathy Reid

My name is Kathy Reid, and I work as an independent digital consultant. I engage with different organisations to do different pieces of work such as analysis, technical white papers, I run hackathons and work with technical communities. You can find me online @KathyReid.


# Overview

SLIDE: Ursula from the Little Mermaid

So, hi! My name is Kathy and I'm a control *enthusiast*. I like controlling things. This talk though won't be about world domination, so if you were here for *that* controlling the future talk, well let's talk later :D *evil fingers*

SLIDE: Overview of topics covered

What we *are* here to talk about today is the future of user interfaces. I'm going to spend a couple of minutes going retro, so we can appreciate just how far we've come in a short period of time. Then, we're going to explore some of the newer user interface approaches that are emerging, such as;

* conversational user interfaces and speech recognition - and I'm going to let you play with Dino here - a toy which uses speech recognition to keep children entertained and learning.

* we'll talk about the rise of natural user interfaces - things like gesture control - everyone remember Minority Report, where Tom Cruise was controlling things on screen like this *demonstrates gesture control*? I've also got my Ninja Sphere here and will show you how that works.

* we're also going to chat a little bit about emotional user interfaces, and neural interfaces, and how they're developing. Who wants to control your devices with the POWER OF YOUR MIND???!!! :-)

* we're going to explore some of the different characteristics, commonalities and differences in user interface development, and do some thinking around what it will mean to control your environment in the future.

## Questions

I'd love to hear your questions, however I think for the ease of microphones and audio, do write them down or remember them, and I'd be delighted to respond to them at the end of the talk.


# History

So, to see where we're going, let's spend a brief moment going back, BACK IN TIME!

SLIDE: Punch cards

Some of you may be seasoned enough to remember punch cards - anyone? With punch cards you literally had to punch holes into these cards, then load a bunch of them into a computer which was about the size of a house, and wait to see if you had a syntax error, then repeat the process when you realised you'd missed a semicolon. *sigh*

Then came these

SLIDE: Old fashioned keyboards

Then came these

SLIDE: Macintosh mouse

And then we had graphics input devices

SLIDE: Wacom graphics tablet

And then we had touch screen devices themselves, such as phones and tablets

SLIDE: ASUS Transformer Trio

This is actually the device I'm using today

So basically the input devices themselves have got incrementally more useful and more sophisticated, we have wireless, we have bluetooth, we have less cords - which is great.

# Trends and the drivers for user interface paradigm shifts

But there's a number of key trends which are driving changes in user interface paradigms. Let's have a look at them.

## Mobile

SLIDE: Lego chains

Firstly, we're not chained to our desks anymore. We work mobile. We have laptops, smartphones, wearables - and if you saw my talk at BuzzConf festival last year, you'll know that soon we'll all have implantables as well!

So we need user interfaces that move with us.

Secondly, we don't want to carry a range of user input devices with us - we're mobile. We're commuting in trains, we're hot desking even if the organisations we work for have actual offices, we're working from coffee shops. We need the user interface to fit where we're working - or playing from - right now.

SLIDE: infra red keyboard

A keyboard that goes where you go!

## Contexts

Another key trend is that we're leading active, on-the-go lifestyles, with our time sliced into ever-thinner wedges. 'Always-on' devices, and work habits mean that the boundaries between work and personal time are blurring.

Twenty years ago you might have got up in the morning, caught a train or bus to work, been at work for 9am, lunched at 1pm, then finished at 5pm, and commuted home. The *context boundaries* - the boundary between what was work context and what was a home context - were very clear.

SLIDE: Slices of different context

In today's society we're *context-switching* freqently. When we wake in the morning - which is usually with the assistance of a mobile device - the first thing we do is check the device for notifications. We might quickly respond to a few work emails to get them out of the way - context switching to work. Then as we're commuting we're in a different context again - where we might do some low value 'quick and easy tasks' on the train. At work we might have the luxury of real deep thinking time - where we don't want notifications or interruptions. At home of an evening we're in a relaxed context, where socialisation with our families is a priority.

So the key theme that I'm drawing out here is that our user interfaces need to change and adapt to the *context* that we're existing in. If we're on a crowded, noisy train, we don't want to use a voice interface - because it's really difficult to do so - it's frustrating - and that means friction - the arch enemy of user interface design.

## Ubiquitous computing

The third key trend that I'd like to explore is *ubiquitous computing*. Just as we're mobile, and exploring all of our different contexts, computing technology is all around us, largely due to the internet of things. Our lights - with things like LIFX - our homes - thanks to things like Nest and Amazon Echo - and even our vehicles - have much more embedded and interconnected technology.

Combine this with both mobility and context-switching, and suddenly we have user interfaces all around us. We plan our day and our journeys on our mobile device. We swipe on with our Myki on the bus or the train. We get coffee or fast food by ordering from a touch screen. Our buildings increasingly have technology, such as proximity sensors and beacons. We're surrounded by user interfaces.



This means that user int


### Too many user interfaces and having to learn them all

This raises a whole raft of very interesting questions. For instance, with so many different user interfaces - touch screens everywhere, elevators that talk to you, and pretty soon - robots that take your order or provide customer service - how many different user interfaces will you have to learn to use? Has anyone switched from a Windows Phone or Apple phone to an Android phone? How different are Cortana and Siri from say Google Now? They use different commands - different conversational UIs - which you have to relearn. So, we need some conversational UI accepted standards.





# Notes
Gartner  from graphical UI to environmental UI - using the environment to control systems
Drivers for change in user interfaces - computing has moved away from the desktop and into mobile devices and wearables - we're no longer at our desks - but the GUI remains, albeit with a touch interface on phones.
Touch interface also seen on wearables - for instance, you tap your fitbit to get information displayed.
What about the Pebble - with the pebble it's still the push of a button to control the interface - but the content is actually driven from a second devices
This could be a good segue into Machine 2 Machine interface - one machine controlling another in response to environmental stimuli.

Adoption - many different people are adopting technology - different languages, different cultural context and different physical abilities - for intance an aging population, who may have difficulty with text, or medical conditions such as stroke which may impair speech.

Sophistication - people are becoming more accepting of technology and are adopting it in many different use cases - they *expect* more user interfaces.

Is this complexity a bad thing? Interfaces are interaction languages - with a syntax and expected response - if I double click this element, then it should respond in this way. Is the explosion of different ways to interface with systems becoming a burden - a cognitive burden? Or will our brains be elastic and just attune or adjust to the propagation of so many different interfaces? Do we need to stop or deprecate some of the interfaces we have to make room?

Evolution of the desktop interface - touch gestures - pinch, zoom, twist
Use of scribes, pens, surface 3 - perhaps our fingers are too fat.
We're still a little way away from the Minority report style gesture control - but with things like the Kinect it's likely to be here soon.

For all the talk of gesture control, tablet control etc, there's still research and development going on established devices - such as keyboards.

Source Citation   (MLA 7th Edition)
Ishii, Hiroshi. "The tangible user interface and its evolution." Communications of the ACM June 2008: 32+. Expanded Academic ASAP. Web. 4 Aug. 2016.
URL
http://ezproxy.deakin.edu.au/login?url=http://go.galegroup.com/ps/i.do?id=GALE%7CA180723121&v=2.1&u=deakin&it=r&p=EAIM&sw=w&asid=fb21fefd9560c521ad947e3f566a958c


Koskela - *pattern control* versus *instant control*
context aware functions - context menus - but now we're seein physical context - environmental UIs.

What happens when environmental UIs and emotional UIs collide?
Conversational UIs that can tell when people are pissed off?
Will the colour of digital signage change to something more soothing?

Trust in the user interface - what happens then the user interface is not just around us, but inside us?

Examples of kickstarter projects for keyboards

https://www.kickstarter.com/projects/keyboardio/the-model-01-an-heirloom-grade-keyboard-for-seriou

https://www.kickstarter.com/projects/1229573443/das-keyboard-5q-the-cloud-connected-keyboard

From Gartner - "The UI shifts from GUIs attached to individual devices to an "environmental user interface," acting as a contextual user access and information delivery engine across multiple devices." - creating a proactive UI framework for the environment

So what are the implications for this for practitioners
* you don't just need a UI framework for something like web or mobile interfaces - your UI framework needs to be an *environmental interaction* framework that has seamless, familiar interfaces for things like touch, gesture control, conversational UI and digital signage, emotional response etc - it's an interaction guide, not a web user interface guide. How is this tied to branding? How many company style guides are still print based, with a concession to web user interfaces? Style guides will become interaction guides - when a sound or gesture or even conversational UI is associated with a brand.

OK Google - Siri - they are part of larger brands.

Just like brands - interaction families will have a personality or flavour - for instance a corporate with a focus on efficiency may have an interaction family which is spartan, quick, with a minimum of movement or sound. A more social or fun brand may have an interaction family which focuses on lots of verbal interaction or fun gestures.

Location awareness and gaze tracking are needed for speech recognition - Microsoft paper - early 2000s

Ninja sphere - gesture control and swiping
Cognitoys Dino - voice recognition - toy interface design
Wiimote - take in the Wiimote to show off

Haber et al - intimate space, personal space, social space, public space - does the user interface know which one it's operating in?

http://developer.affectiva.com/ - emotional API







# Overview

# References
