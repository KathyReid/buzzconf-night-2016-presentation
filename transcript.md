# Controlling the Future - BuzzConf Nights August 2016

## Welcome to country

I wish to begin by acknowledging the Wurundjeri people, the traditional owners of the land on which we are gathered today. We pay our respects to the local people for allowing us to have our gathering on their land and to their Elders; past, present and future.

Thanks everyone for coming today, and a huge thanks to Ben and Rick for convening an event as amazing as Buzzconf Nights.

## Audio impaired

If there are any hearing impaired members of the audience, let me know and I can provide you with a transcript of the presentation to follow along to.

## License

This presentation is licensed under Creative Commons universal - you're free to share and re-use this presentation. Please use it to do awesome things.

## Kathy Reid

My name is Kathy Reid, and I work as an independent digital consultant. I engage with different organisations to do different pieces of work such as analysis, technical white papers, I run hackathons and work with technical communities. You can find me online @KathyReid.

# Overview

SLIDE: Ursula from the Little Mermaid

So, hi! I'm a control *enthusiast*. I like controlling things. This talk though won't be about world domination, so if you were here for *that* controlling the future talk, well let's talk later :D *evil fingers*

SLIDE: Overview of topics covered

What we *are* here to talk about today is the future of user interfaces. I'm going to spend a couple of minutes going retro, so we can appreciate just how far we've come in a short period of time. Then, we're going to explore some of the newer user interface approaches that are emerging, such as;

* conversational user interfaces and speech recognition - and I'm going to let you play with Dino here - a toy which uses speech recognition to keep children entertained and learning.

* we'll talk about the rise of natural user interfaces - things like gesture control - everyone remember Minority Report, where Tom Cruise was controlling things on screen like this *demonstrates gesture control*? I've also got my Ninja Sphere here and will show you how that works.

* we're also going to chat a little bit about emotional user interfaces, and neural interfaces, and how they're developing. Who wants to control your devices with the POWER OF YOUR MIND???!!! :-)

* we're going to explore some of the different characteristics, commonalities and differences in user interface development, and do some thinking around what it will mean to control your environment in the future.

## Questions

I'd love to hear your questions, however I think for the ease of microphones and audio, do write them down or remember them, and I'd be delighted to respond to them at the end of the talk.


# History

So, to see where we're going, let's spend a brief moment going back, BACK IN TIME!

SLIDE: Punch cards

Some of you may be seasoned enough to remember punch cards - anyone? With punch cards you literally had to punch holes into these cards, then load a bunch of them into a computer which was about the size of a house, and wait to see if you had a syntax error, then repeat the process when you realised you'd missed a semicolon. *sigh*

Then came these

SLIDE: Old fashioned keyboards

Then came these

SLIDE: Macintosh mouse

And then we had graphics input devices

SLIDE: Wacom graphics tablet

And we saw advancing in entertainment and gaming technology as well

SLIDE: Nintendo WiiMote

And then we had touch screen devices themselves, such as phones and tablets

SLIDE: ASUS Transformer Trio

This is actually the device I'm using today

So basically the input devices themselves have got incrementally more useful and more sophisticated, we have wireless, we have bluetooth, we have less cords - which is great.

# Trends and the drivers for user interface paradigm shifts

But there's a number of key trends which are driving changes in user interface paradigms - paradigm shifts as opposed to incremental changes. Let's have a look at them.

## Mobile

SLIDE: Lego chains

Firstly, we're not chained to our desks anymore. We work mobile. We have laptops, smartphones, wearables - and if you saw my talk at BuzzConf festival last year, you'll know that soon we'll all have implantables as well!

So we need user interfaces that move with us.

Secondly, we don't want to carry a range of user input devices with us - we're mobile. We're commuting in trains, we're hot desking even if the organisations we work for have actual offices, we're working from coffee shops. We need the user interface to fit where we're working - or playing from - right now.

SLIDE: infra red keyboard

A keyboard that goes where you go!

## Contexts

Another key trend is that we're leading active, on-the-go lifestyles, with our time sliced into ever-thinner wedges. 'Always-on' devices, and work habits mean that the boundaries between work and personal time are blurring.

Twenty years ago you might have got up in the morning, caught a train or bus to work, been at work for 9am, lunched at 1pm, then finished at 5pm, and commuted home. The *context boundaries* - the boundary between what was work context and what was a home context - were very clear.

SLIDE: Slices of different context

In today's society we're *context-switching* freqently. When we wake in the morning - which is usually with the assistance of a mobile device - the first thing we do is check the device for notifications. We might quickly respond to a few work emails to get them out of the way - context switching to work. Then as we're commuting we're in a different context again - where we might do some low value 'quick and easy tasks' on the train. At work we might have the luxury of real deep thinking time - where we don't want notifications or interruptions. At home of an evening we're in a relaxed context, where socialisation with our families is a priority.

So the key theme that I'm drawing out here is that our user interfaces need to change and adapt to the *context* that we're existing in. If we're on a crowded, noisy train, we don't want to use a voice interface - because it's really difficult to do so - it's frustrating - and that means friction - the arch enemy of user interface design.

## Ubiquitous computing

SLIDE: LIFX - technology is all around us

The third key trend that I'd like to explore is *ubiquitous computing*. Just as we're mobile, and exploring all of our different contexts, computing technology is all around us, largely due to the internet of things. Our lights - with things like LIFX - our homes - thanks to things like Nest and Amazon Echo - and even our vehicles - have much more embedded and interconnected technology.

SLIDE: Tesla Model S - vehicle technology

Combine this with both mobility and context-switching, and suddenly we have user interfaces all around us. We plan our day and our journeys on our mobile device. We swipe on with our Myki on the bus or the train. We get coffee or fast food by ordering from a touch screen. Our buildings increasingly have technology, such as proximity sensors and beacons. We're surrounded by user interfaces - it's not just computing that's ubiquitous, user interfaces are ubiquitous too.

## Key areas of user interface development

So, now that we've explored some of the overall trends that are shaping user interface technology, let's take a look at some of the emerging technologies in this space.

### Speech recognition and voice control

Without a doubt, one of the hottest technologies to emerge in this space has been speech recognition and voice control. This technology has been around for decades now, but was has been held back by a number of challenges. There are about a million words in the English language - give or take a few thousand. Now try imagining first capturing those words - perhaps in a busy cafe or where there's lots of background noise. like an open plan office. Then you need to account for intonation or accent or _emphasis on the wrong syllable_. The permutations and combinations quickly multiply into a very complex problem set.

The way that speech recognition on mobile devices - Siri, Cortana, Google Now and so on aim to address this issue is by reducing the *problem space*. You know how there are a limited number of commands that you can issue to Google Now - you might ask about the weather or ask Google to add pasta sauce to your shopping list, or ask a 'who was' question. Right - so you're not using the million or so words in the English language - you're using a much smaller subset that's easier to predict - particularly if you factor in *context* - such as being geographically in a shopping centre, or near a train station.

So, we're going to see the rise of speech recognition and voice commands, but for a few years they're still going to be limited in their vocabulary and what you can do with them.

However, even with a limited vocabulary there are lots of possibilities for how speech recognition and voice commands will be incorporated into technology.

#### Cognitoys Dino

SLIDE: Cognitoys Dino

One of those is *toys*. We're now starting to see toys, such as the Elemental Path Cognitoys Dino, use voice recognition combined with things like artificial intelligence to provide both a learning and entertainment experience.

DEMONSTRATION of the Cognitoys Dino

So what I'd like to do now is give you a run through of the Dino - which uses voice recognition and the IBM Watson platform to answer questions, tell jokes and help craft stories.

* How old are you?
* Do you have any brothers and sisters?
* What is a computer?

(let the audience have a play with it)

### Conversational UI

So, did you see some of the patterns that were emerging as we interacted with Dino? That's called a *conversational UI* - and you might have used one of these with tools like Slack or IRC in the past. Again, using a limited vocabulary, you're able to interact in some ways with *bots* or programs just as if you were interacting with a human.

If we combine this with other trends - such as self service touch screens and kiosks in places like airports and fast food restaurants, then you can see a natural progression - you won't just have to touch for your order, you can place it as you would going through drive through - except the agent on the other side of the microphone won't be a human, they'll be an Ordering Bot :D

### Implications for consideration

Of course, having listening devices around you can be a little unnerving. And there's some machine ethics issues here - and I know machine ethics is one of Paul's favourite topics.

SLIDE: Mirrored drives

* For instance, with the CogniToys dino, what happens if the child who's using it divulges private or sensitive information? What if that information indicates they're being abused? What obligation does the platform provider hold?

* How do you balance privacy with safety and security considerations? Is it feasible to think that national security agencies may want to capture or hold on to some of these conversations - for instance if the platform identifies the user as being from a particular ethnic group?

* And there's a fair body of research around the types of voices we want to interact with - as humans we prefer female voices to male voices. But will having conversational UIs with predominantly female voices further condition us as a society into believing that women are to serve, rather than to be served?

I'll leave you to ponder that philosophical reflection.

## Gesture control and other natural user interfaces

SLIDE: Gesture control

The next emerging type of user interface technology that we're starting to see is in the field of gesture control and *natural user interfaces*. Most of you will have used gestures such as flick, pinch, zoom, tap and double tap to use table and smart phone applications, and natural user interfaces are really an extension to this - gestures without a touch screen.

Has anyone seen Minority Report - where the character played by Tom Cruise is using gestures like swipe and pinch and zoom, as part of the user interface? That's the sort of thing we're talking about here.

And it's closer than you might think.

### LEAP Motion and Kinect

SLIDE: 





# Implications for consideration

### User interfaces that talk to each other




### Too many user interfaces and having to learn them all

This raises a whole raft of very interesting questions. For instance, with so many different user interfaces - touch screens everywhere, elevators that talk to you, and pretty soon - robots that take your order or provide customer service - how many different user interfaces will you have to learn to use? Has anyone switched from a Windows Phone or Apple phone to an Android phone? How different are Cortana and Siri from say Google Now? They use different commands - different conversational UIs - which you have to relearn. So, we need some conversational UI accepted standards.





# Notes
Gartner  from graphical UI to environmental UI - using the environment to control systems
Drivers for change in user interfaces - computing has moved away from the desktop and into mobile devices and wearables - we're no longer at our desks - but the GUI remains, albeit with a touch interface on phones.
Touch interface also seen on wearables - for instance, you tap your fitbit to get information displayed.
What about the Pebble - with the pebble it's still the push of a button to control the interface - but the content is actually driven from a second devices
This could be a good segue into Machine 2 Machine interface - one machine controlling another in response to environmental stimuli.

Adoption - many different people are adopting technology - different languages, different cultural context and different physical abilities - for intance an aging population, who may have difficulty with text, or medical conditions such as stroke which may impair speech.

Sophistication - people are becoming more accepting of technology and are adopting it in many different use cases - they *expect* more user interfaces.

Is this complexity a bad thing? Interfaces are interaction languages - with a syntax and expected response - if I double click this element, then it should respond in this way. Is the explosion of different ways to interface with systems becoming a burden - a cognitive burden? Or will our brains be elastic and just attune or adjust to the propagation of so many different interfaces? Do we need to stop or deprecate some of the interfaces we have to make room?

Evolution of the desktop interface - touch gestures - pinch, zoom, twist
Use of scribes, pens, surface 3 - perhaps our fingers are too fat.
We're still a little way away from the Minority report style gesture control - but with things like the Kinect it's likely to be here soon.

For all the talk of gesture control, tablet control etc, there's still research and development going on established devices - such as keyboards.

Source Citation   (MLA 7th Edition)
Ishii, Hiroshi. "The tangible user interface and its evolution." Communications of the ACM June 2008: 32+. Expanded Academic ASAP. Web. 4 Aug. 2016.
URL
http://ezproxy.deakin.edu.au/login?url=http://go.galegroup.com/ps/i.do?id=GALE%7CA180723121&v=2.1&u=deakin&it=r&p=EAIM&sw=w&asid=fb21fefd9560c521ad947e3f566a958c


Koskela - *pattern control* versus *instant control*
context aware functions - context menus - but now we're seein physical context - environmental UIs.

What happens when environmental UIs and emotional UIs collide?
Conversational UIs that can tell when people are pissed off?
Will the colour of digital signage change to something more soothing?

Trust in the user interface - what happens then the user interface is not just around us, but inside us?

Examples of kickstarter projects for keyboards

https://www.kickstarter.com/projects/keyboardio/the-model-01-an-heirloom-grade-keyboard-for-seriou

https://www.kickstarter.com/projects/1229573443/das-keyboard-5q-the-cloud-connected-keyboard

From Gartner - "The UI shifts from GUIs attached to individual devices to an "environmental user interface," acting as a contextual user access and information delivery engine across multiple devices." - creating a proactive UI framework for the environment

So what are the implications for this for practitioners
* you don't just need a UI framework for something like web or mobile interfaces - your UI framework needs to be an *environmental interaction* framework that has seamless, familiar interfaces for things like touch, gesture control, conversational UI and digital signage, emotional response etc - it's an interaction guide, not a web user interface guide. How is this tied to branding? How many company style guides are still print based, with a concession to web user interfaces? Style guides will become interaction guides - when a sound or gesture or even conversational UI is associated with a brand.

OK Google - Siri - they are part of larger brands.

Just like brands - interaction families will have a personality or flavour - for instance a corporate with a focus on efficiency may have an interaction family which is spartan, quick, with a minimum of movement or sound. A more social or fun brand may have an interaction family which focuses on lots of verbal interaction or fun gestures.

Location awareness and gaze tracking are needed for speech recognition - Microsoft paper - early 2000s

Ninja sphere - gesture control and swiping
Cognitoys Dino - voice recognition - toy interface design
Wiimote - take in the Wiimote to show off

Haber et al - intimate space, personal space, social space, public space - does the user interface know which one it's operating in?

http://developer.affectiva.com/ - emotional API







# Overview

# References
